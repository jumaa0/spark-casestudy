{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fdbfa5f-7b41-430a-b0c3-8125ae299a84",
   "metadata": {},
   "source": [
    "# Get functions from etl_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c18c2d3-9f94-4882-965f-473ffd0a1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "def create_spark_session(app_name=\"casestudy\"):\n",
    "    return SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"/path/to/warehouse\") \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://localhost:9083\") \\\n",
    "        .config(\"spark.hadoop.hive.metastore.warehouse.dir\", \"/path/to/hive/warehouse\") \\\n",
    "        .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "        .config(\"spark.eventLog.logBlockUpdates.enabled\", True) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def read_csv_to_df(spark, file_path, schema=None, sep=\",\", infer_schema=False, header=True):\n",
    "    hdfs_path = \"hdfs://localhost:9000\" + file_path\n",
    "    \n",
    "    if infer_schema:\n",
    "        df = spark.read.csv(hdfs_path, sep=sep, header=header, inferSchema=True)\n",
    "    else:\n",
    "        df = spark.read.csv(hdfs_path, schema=schema, sep=sep, header=header)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    return df.dropDuplicates()\n",
    "\n",
    "def check_nulls(df, column_name):\n",
    "    return df.filter(col(column_name).isNull()).count() > 0\n",
    "\n",
    "def calculate_total_paid_price_after_discount(transactions_df):\n",
    "    return transactions_df.withColumn(\n",
    "        \"total_paid_price_after_discount\",\n",
    "        when(col(\"offer_1\") == \"1\", col(\"unit_price\") * 0.15)\n",
    "        .when(col(\"offer_2\") == \"2\", col(\"unit_price\") * 0.25)\n",
    "        .when(col(\"offer_3\") == \"3\", col(\"unit_price\") * 0.35)\n",
    "        .when(col(\"offer_4\") == \"4\", col(\"unit_price\") * 0.45)\n",
    "        .when(col(\"offer_5\") == \"5\", col(\"unit_price\") * 0.50)\n",
    "        .otherwise(col(\"unit_price\"))\n",
    "    )\n",
    "\n",
    "def add_offer_column(transactions_df):\n",
    "    return transactions_df.withColumn(\n",
    "        \"offer\",\n",
    "        when(col(\"offer_1\").isNotNull(), \"1\")\n",
    "        .when(col(\"offer_2\").isNotNull(), \"2\")\n",
    "        .when(col(\"offer_3\").isNotNull(), \"3\")\n",
    "        .when(col(\"offer_4\").isNotNull(), \"4\")\n",
    "        .when(col(\"offer_5\").isNotNull(), \"5\")\n",
    "    )\n",
    "\n",
    "\n",
    "def insert_into_hive_table(spark, df, table_name, table_location=None, primary_key=None):\n",
    "    \"\"\"\n",
    "    Insert data from a DataFrame into a Hive table.\n",
    "\n",
    "    Parameters:\n",
    "    - spark: SparkSession object\n",
    "    - df: Spark DataFrame\n",
    "    - table_name: Name of the Hive table\n",
    "    - table_location: Location of the external table (optional)\n",
    "    - primary_key: Primary key column(s) to identify new records (default is None)\n",
    "    \"\"\"\n",
    "    \n",
    "    table_exists = spark._jsparkSession.catalog().tableExists(table_name)\n",
    "\n",
    "    if table_name in [\"casestudy.fact_sales\", \"casestudy.dim_sales\"]:\n",
    "        print(f\"Inserting data into {table_name}.\")\n",
    "        df.write.mode('append').insertInto(table_name)\n",
    "    else:\n",
    "        if table_exists:\n",
    "            print(f\"Table {table_name} already exists. Inserting only new records.\")\n",
    "            existing_data = spark.table(table_name)\n",
    "\n",
    "            if primary_key:\n",
    "                new_data = df.join(existing_data, on=primary_key, how=\"left_anti\")\n",
    "                if new_data.count() > 0:\n",
    "                    new_data.write.mode('append').saveAsTable(table_name)\n",
    "                    print(f\"Inserted {new_data.count()} new records into {table_name}.\")\n",
    "                else:\n",
    "                    print(f\"No new records to insert into {table_name}.\")\n",
    "            else:\n",
    "                print(f\"Primary key is required to identify new records for table {table_name}.\")\n",
    "        else:\n",
    "            print(f\"Table {table_name} does not exist. Creating and inserting data.\")\n",
    "            df.write.mode('overwrite').saveAsTable(table_name)\n",
    "\n",
    "def create_fact_sales(transactions_df):\n",
    "    return transactions_df.select(\n",
    "        \"transaction_id\", \"customer_id\", \"sales_agent_id\", \"branch_id\", \"product_id\", \"offer\", \"units\", \"unit_price\", \"total_paid_price_after_discount\"\n",
    "    )\n",
    "\n",
    "def create_dim_sales(transactions_df):\n",
    "    return transactions_df.select(\n",
    "        \"transaction_id\",\n",
    "        col(\"transaction_date\").cast(\"timestamp\").alias(\"transaction_date\"),\n",
    "        \"is_online\",\n",
    "        \"payment_method\",\n",
    "        \"shipping_address\",\n",
    "        col(\"load_date\").cast(\"timestamp\").alias(\"load_date\"),\n",
    "        \"load_source\"\n",
    "    )\n",
    "\n",
    "def create_dim_product(transactions_df):\n",
    "    return transactions_df.select(\n",
    "        \"product_id\",\n",
    "        \"product_name\",\n",
    "        \"product_category\",\n",
    "        col(\"load_date\").cast(\"timestamp\").alias(\"load_date\"),\n",
    "        \"load_source\"\n",
    "    ).distinct()\n",
    "\n",
    "def create_dim_customer(transactions_df):\n",
    "    schema = StructType([\n",
    "        StructField(\"customer_id\", IntegerType(), True),\n",
    "        StructField(\"customer_fname\", StringType(), True),\n",
    "        StructField(\"customer_lname\", StringType(), True),\n",
    "        StructField(\"customer_email\", StringType(), True),\n",
    "        StructField(\"load_date\", TimestampType(), True),\n",
    "        StructField(\"load_source\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    return transactions_df.select(\n",
    "        \"customer_id\",\n",
    "        \"customer_fname\",\n",
    "        \"cusomter_lname\",\n",
    "        \"cusomter_email\",\n",
    "        col(\"load_date\").cast(\"timestamp\").alias(\"load_date\"),\n",
    "        \"load_source\"\n",
    "    ).distinct()\n",
    "\n",
    "def create_dim_branch(branches_df):\n",
    "    return branches_df.select(\n",
    "        \"branch_id\",\n",
    "        \"location\",\n",
    "        col(\"establish_date\").cast(TimestampType()).alias(\"establish_date\"),\n",
    "        \"class\",\n",
    "        col(\"load_date\").cast(TimestampType()).alias(\"load_date\"),\n",
    "        \"load_source\"\n",
    "    ).distinct()\n",
    "\n",
    "def create_dim_agent(agents_df):\n",
    "    # Cleanse null values and handle column references correctly\n",
    "    cleaned_agents_df = agents_df.withColumn(\"hire_date\", when(col(\"hire_date\") == \"\", None).otherwise(col(\"hire_date\"))) \\\n",
    "                                .withColumn(\"load_date\", when(col(\"load_date\") == \"\", None).otherwise(col(\"load_date\")))\n",
    "\n",
    "    return cleaned_agents_df.select(\n",
    "        col(\"sales_person_id\"),\n",
    "        col(\"name\").alias(\"sales_agent_name\"),  # Ensure correct column reference here\n",
    "        col(\"hire_date\").cast(TimestampType()).alias(\"hire_date\"),\n",
    "        col(\"load_date\").cast(TimestampType()).alias(\"load_date\"),\n",
    "        col(\"load_source\")\n",
    "    ).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f1931-b047-4973-9157-e2035786dc79",
   "metadata": {},
   "source": [
    "# Run the main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32a72f18-bc31-4859-a406-705d07a09188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, BooleanType\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, BooleanType\n",
    "\n",
    "spark = create_spark_session()\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, BooleanType\n",
    "\n",
    "# Define schema for transactions DataFrame\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"transaction_date\", StringType(), True),\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_fname\", StringType(), True),\n",
    "    StructField(\"cusomter_lname\", StringType(), True),\n",
    "    StructField(\"cusomter_email\", StringType(), True),\n",
    "    StructField(\"sales_agent_id\", StringType(), True),\n",
    "    StructField(\"branch_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"product_category\", StringType(), True),\n",
    "    StructField(\"offer_1\", StringType(), True),\n",
    "    StructField(\"offer_2\", StringType(), True),\n",
    "    StructField(\"offer_3\", StringType(), True),\n",
    "    StructField(\"offer_4\", StringType(), True),\n",
    "    StructField(\"offer_5\", StringType(), True),\n",
    "    StructField(\"units\", StringType(), True),\n",
    "    StructField(\"unit_price\", StringType(), True),\n",
    "    StructField(\"is_online\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"shipping_address\", StringType(), True),\n",
    "    StructField(\"load_date\", StringType(), True),\n",
    "    StructField(\"load_source\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for branches DataFrame\n",
    "branches_schema = StructType([\n",
    "    StructField(\"branch_id\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"establish_date\", StringType(), True),\n",
    "    StructField(\"class\", StringType(), True),\n",
    "    StructField(\"load_date\", StringType(), True),\n",
    "    StructField(\"load_source\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for agents DataFrame\n",
    "agents_schema = StructType([\n",
    "    StructField(\"sales_person_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"hire_date\", StringType(), True),\n",
    "    StructField(\"load_date\", StringType(), True),\n",
    "    StructField(\"load_source\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Reading CSV files into DataFrames with inferred schemas\n",
    "transactions_df = read_csv_to_df(spark, \"/casestudy/day183/hour14/sales_transactions.csv\", schema=transactions_schema)\n",
    "branches_df = read_csv_to_df(spark, \"/casestudy/day183/hour14/branches.csv\", schema=branches_schema)\n",
    "agents_df = read_csv_to_df(spark, \"/casestudy/day183/hour14/sales_agents.csv\", schema=agents_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153ca4f5-b2e6-4140-a0d5-a1a7b639c8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+-----------+--------------+--------------+--------------------+--------------+---------+----------+------------+----------------+-------+-------+-------+-------+-------+-----+----------+---------+--------------+----------------+----------+-----------+\n",
      "|transaction_date|  transaction_id|customer_id|customer_fname|cusomter_lname|      cusomter_email|sales_agent_id|branch_id|product_id|product_name|product_category|offer_1|offer_2|offer_3|offer_4|offer_5|units|unit_price|is_online|payment_method|shipping_address| load_date|load_source|\n",
      "+----------------+----------------+-----------+--------------+--------------+--------------------+--------------+---------+----------+------------+----------------+-------+-------+-------+-------+-------+-----+----------+---------+--------------+----------------+----------+-----------+\n",
      "|      2023-10-25|trx-072037549384|      85550|          Emma|        Wilson|emma.wilson@outlo...|           2.0|      3.0|         3|      Tablet|     Electronics|   null|   null|   null|   null|   null|    7|    299.99|       no|          Cash|            null|2024-07-01|    source1|\n",
      "|        2022-5-8|trx-125208155197|      85512|        Olivia|        Miller|olivia.miller@out...|           9.0|      5.0|        11|          TV|     Electronics|   null|   null|   null|   null|   null|    3|    899.99|       no|   Credit Card|            null|2024-07-01|    source1|\n",
      "|        2023-5-3|trx-667682345565|      85512|        Olivia|        Miller|olivia.miller@out...|           7.0|      2.0|        10|     Sandals|        Footwear|   null|   null|   null|   True|   null|    3|     39.99|       no|   Credit Card|            null|2024-07-01|    source1|\n",
      "|        2022-1-9|trx-706068352444|      85526|        Sophia|         Smith|sophia.smith@hotm...|           2.0|      4.0|         2|  Smartphone|     Electronics|   null|   null|   null|   null|   True|    7|    699.99|       no|   Credit Card|            null|2024-07-01|    source1|\n",
      "|      2022-12-10|trx-040134528974|      85535|       Michael|         Davis|michael.davis@out...|           9.0|      3.0|        16|       Skirt|        Clothing|   null|   null|   null|   True|   null|    7|     39.99|       no|          Cash|            null|2024-07-01|    source1|\n",
      "+----------------+----------------+-----------+--------------+--------------+--------------------+--------------+---------+----------+------------+----------------+-------+-------+-------+-------+-------+-----+----------+---------+--------------+----------------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d69c21a4-76ec-4f48-939e-de68acf19eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates\n",
    "transactions_df = remove_duplicates(transactions_df)\n",
    "branches_df = remove_duplicates(branches_df)\n",
    "agents_df = remove_duplicates(agents_df)\n",
    "\n",
    "# Checking for null values in key columns\n",
    "if check_nulls(transactions_df, \"transaction_id\"):\n",
    "    print(\"Null values found in transaction_id column\")\n",
    "if check_nulls(branches_df, \"branch_id\"):\n",
    "    print(\"Null values found in branch_id column\")\n",
    "if check_nulls(agents_df, \"sales_person_id\"):\n",
    "    print(\"Null values found in sales_person_id column\")\n",
    "\n",
    "# Calculating total paid price after discount and adding offer column\n",
    "transactions_df = calculate_total_paid_price_after_discount(transactions_df)\n",
    "transactions_df = add_offer_column(transactions_df)\n",
    "\n",
    "# Creating fact and dimension tables\n",
    "fact_sales = create_fact_sales(transactions_df)\n",
    "dim_sales = create_dim_sales(transactions_df)\n",
    "dim_product = create_dim_product(transactions_df)\n",
    "dim_customer = create_dim_customer(transactions_df)\n",
    "dim_branch = create_dim_branch(branches_df)\n",
    "dim_agent = create_dim_agent(agents_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2675c2-3dea-40c0-9e0a-6cb56b85c5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+--------------+---------+----------+-----+-----+----------+-------------------------------+\n",
      "|  transaction_id|customer_id|sales_agent_id|branch_id|product_id|offer|units|unit_price|total_paid_price_after_discount|\n",
      "+----------------+-----------+--------------+---------+----------+-----+-----+----------+-------------------------------+\n",
      "|trx-829015484650|      85500|           8.0|      1.0|        30| null|    2|     24.99|                          24.99|\n",
      "|trx-318134583182|      85484|           6.0|      1.0|        24|    1|    8|     49.99|                          49.99|\n",
      "|trx-738773442038|      85557|          10.0|      4.0|        13|    1|    3|    149.99|                         149.99|\n",
      "|trx-562759580036|      85532|          null|     null|        18|    4|   10|    149.99|                         149.99|\n",
      "|trx-641002331340|      85551|          null|     null|        22| null|    1|     79.99|                          79.99|\n",
      "+----------------+-----------+--------------+---------+----------+-----+-----+----------+-------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5f2344f-7f73-478a-8642-0b1239060f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+---------+--------------+--------------------+-------------------+-----------+\n",
      "|  transaction_id|   transaction_date|is_online|payment_method|    shipping_address|          load_date|load_source|\n",
      "+----------------+-------------------+---------+--------------+--------------------+-------------------+-----------+\n",
      "|trx-829015484650|2023-12-11 00:00:00|       no|   Credit Card|                null|2024-07-01 00:00:00|    source1|\n",
      "|trx-318134583182|2022-07-11 00:00:00|       no|          Cash|                null|2024-07-01 00:00:00|    source1|\n",
      "|trx-738773442038|2023-11-10 00:00:00|       no|          Cash|                null|2024-07-01 00:00:00|    source1|\n",
      "|trx-562759580036|2023-04-16 00:00:00|      yes|        PayPal|22 Eden Street/Mi...|2024-07-01 00:00:00|    source1|\n",
      "|trx-641002331340|2023-11-16 00:00:00|      yes|        Stripe|5124 Williston Ro...|2024-07-01 00:00:00|    source1|\n",
      "+----------------+-------------------+---------+--------------+--------------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_sales.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "901cbf25-c166-4db1-8f82-3602ad3904e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data into casestudy.fact_sales.\n",
      "Inserting data into casestudy.dim_sales.\n"
     ]
    }
   ],
   "source": [
    "# Inserting data into Hive tables\n",
    "    insert_into_hive_table(spark, fact_sales, \"casestudy.fact_sales\")  # Fact sales - directly insert\n",
    "    insert_into_hive_table(spark, dim_sales, \"casestudy.dim_sales\")    # Dim sales - directly insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1233c2b5-8c4b-4647-aed1-2e17519e088d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
