{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c18c2d3-9f94-4882-965f-473ffd0a1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# etl_functions.py\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "def create_spark_session(app_name=\"casestudy\"):\n",
    "    return SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"/path/to/warehouse\") \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://localhost:9083\") \\\n",
    "        .config(\"spark.hadoop.hive.metastore.warehouse.dir\", \"/path/to/hive/warehouse\") \\\n",
    "        .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "        .config(\"spark.eventLog.logBlockUpdates.enabled\", True) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def read_csv_to_df(spark, file_path, schema=None, sep=\",\", infer_schema=False, header=True):\n",
    "    hdfs_path = \"hdfs://localhost:9000\" + file_path\n",
    "    \n",
    "    if infer_schema:\n",
    "        df = spark.read.csv(hdfs_path, sep=sep, header=header, inferSchema=True)\n",
    "    else:\n",
    "        df = spark.read.csv(hdfs_path, schema=schema, sep=sep, header=header)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    return df.dropDuplicates()\n",
    "\n",
    "def check_nulls(df, column_name):\n",
    "    return df.filter(col(column_name).isNull()).count() > 0\n",
    "\n",
    "def calculate_total_paid_price_after_discount(transactions_df):\n",
    "    return transactions_df.withColumn(\n",
    "        \"total_paid_price_after_discount\",\n",
    "        when(col(\"offer_1\") == \"1\", col(\"unit_price\") * 0.15)\n",
    "        .when(col(\"offer_2\") == \"2\", col(\"unit_price\") * 0.25)\n",
    "        .when(col(\"offer_3\") == \"3\", col(\"unit_price\") * 0.35)\n",
    "        .when(col(\"offer_4\") == \"4\", col(\"unit_price\") * 0.45)\n",
    "        .when(col(\"offer_5\") == \"5\", col(\"unit_price\") * 0.50)\n",
    "        .otherwise(col(\"unit_price\"))\n",
    "    )\n",
    "\n",
    "def add_offer_column(transactions_df):\n",
    "    return transactions_df.withColumn(\n",
    "        \"offer\",\n",
    "        when(col(\"offer_1\"), \"1\")\n",
    "        .when(col(\"offer_2\"), \"2\")\n",
    "        .when(col(\"offer_3\"), \"3\")\n",
    "        .when(col(\"offer_4\"), \"4\")\n",
    "        .when(col(\"offer_5\"), \"5\")\n",
    "    )\n",
    "\n",
    "def insert_into_hive_table(spark, df, table_name, table_location=None, primary_key=None):\n",
    "    \"\"\"\n",
    "    Insert data from a DataFrame into a Hive table.\n",
    "\n",
    "    Parameters:\n",
    "    - spark: SparkSession object\n",
    "    - df: Spark DataFrame\n",
    "    - table_name: Name of the Hive table\n",
    "    - table_location: Location of the external table (optional)\n",
    "    - primary_key: Primary key column(s) to identify new records (default is None)\n",
    "    \"\"\"\n",
    "    if not table_location:\n",
    "        table_location = \"/database\"  # Set a default location if not provided\n",
    "\n",
    "    table_path = f\"{table_location}/{table_name}\"\n",
    "    table_exists = spark._jsparkSession.catalog().tableExists(table_name)\n",
    "\n",
    "    if table_name in [\"casestudy.fact_sales\", \"dim_sales\"]:\n",
    "        print(f\"Inserting data into {table_name}.\")\n",
    "        df.write.mode('append').insertInto(table_name)\n",
    "    else:\n",
    "        if table_exists:\n",
    "            print(f\"Table {table_name} already exists. Inserting only new records.\")\n",
    "            existing_data = spark.table(table_name)\n",
    "\n",
    "            if primary_key:\n",
    "                new_data = df.join(existing_data, on=primary_key, how=\"left_anti\")\n",
    "                if new_data.count() > 0:\n",
    "                    new_data.write.mode('append').insertInto(table_name)\n",
    "                    print(f\"Inserted {new_data.count()} new records into {table_name}.\")\n",
    "                else:\n",
    "                    print(f\"No new records to insert into {table_name}.\")\n",
    "            else:\n",
    "                print(f\"Primary key is required to identify new records for table {table_name}.\")\n",
    "        else:\n",
    "            print(f\"Table {table_name} does not exist. Creating and inserting data.\")\n",
    "            df.write.mode('overwrite').saveAsTable(table_name)\n",
    "\n",
    "def create_fact_sales(transactions_df):\n",
    "    return transactions_df.select(\n",
    "        \"transaction_id\", \"customer_id\", \"sales_agent_id\", \"branch_id\", \"product_id\", \"offer\", \"units\", \"unit_price\", \"total_paid_price_after_discount\"\n",
    "    )\n",
    "\n",
    "def create_dim_sales(transactions_df):\n",
    "    return transactions_df.select(\n",
    "        \"transaction_id\",\n",
    "        col(\"transaction_date\").cast(\"timestamp\").alias(\"transaction_date\"),\n",
    "        \"is_online\",\n",
    "        \"payment_method\",\n",
    "        \"shipping_address\",\n",
    "        col(\"load_date\").cast(\"timestamp\").alias(\"load_date\"),\n",
    "        \"load_source\"\n",
    "    )\n",
    "\n",
    "def create_dim_product(transactions_df):\n",
    "    return transactions_df.select(\n",
    "        \"product_id\",\n",
    "        \"product_name\",\n",
    "        \"product_category\",\n",
    "        col(\"load_date\").cast(\"timestamp\").alias(\"load_date\"),\n",
    "        \"load_source\"\n",
    "    ).distinct()\n",
    "\n",
    "def create_dim_customer(transactions_df):\n",
    "    schema = StructType([\n",
    "        StructField(\"customer_id\", IntegerType(), True),\n",
    "        StructField(\"customer_fname\", StringType(), True),\n",
    "        StructField(\"customer_lname\", StringType(), True),\n",
    "        StructField(\"customer_email\", StringType(), True),\n",
    "        StructField(\"load_date\", TimestampType(), True),\n",
    "        StructField(\"load_source\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    return transactions_df.select(\n",
    "        \"customer_id\",\n",
    "        \"customer_fname\",\n",
    "        \"cusomter_lname\",\n",
    "        \"cusomter_email\",\n",
    "        col(\"load_date\").cast(\"timestamp\").alias(\"load_date\"),\n",
    "        \"load_source\"\n",
    "    ).distinct()\n",
    "\n",
    "def create_dim_branch(branches_df):\n",
    "    return branches_df.select(\n",
    "        \"branch_id\",\n",
    "        \"location\",\n",
    "        col(\"establish_date\").cast(TimestampType()).alias(\"establish_date\"),\n",
    "        \"class\",\n",
    "        col(\"load_date\").cast(TimestampType()).alias(\"load_date\"),\n",
    "        \"load_source\"\n",
    "    ).distinct()\n",
    "\n",
    "def create_dim_agent(agents_df):\n",
    "    # Cleanse null values and handle column references correctly\n",
    "    cleaned_agents_df = agents_df.withColumn(\"hire_date\", when(col(\"hire_date\") == \"\", None).otherwise(col(\"hire_date\"))) \\\n",
    "                                .withColumn(\"load_date\", when(col(\"load_date\") == \"\", None).otherwise(col(\"load_date\")))\n",
    "\n",
    "    return cleaned_agents_df.select(\n",
    "        col(\"sales_person_id\"),\n",
    "        col(\"name\").alias(\"sales_agent_name\"),  # Ensure correct column reference here\n",
    "        col(\"hire_date\").cast(TimestampType()).alias(\"hire_date\"),\n",
    "        col(\"load_date\").cast(TimestampType()).alias(\"load_date\"),\n",
    "        col(\"load_source\")\n",
    "    ).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32a72f18-bc31-4859-a406-705d07a09188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, BooleanType\n",
    "\n",
    "spark = create_spark_session()\n",
    "\n",
    "# Reading CSV files into DataFrames with inferred schemas\n",
    "transactions_df = read_csv_to_df(spark, \"/casestudy/day176/hour20/sales_transactions.csv\", infer_schema=True)\n",
    "branches_df = read_csv_to_df(spark, \"/casestudy/day176/hour20/branches.csv\", infer_schema=True)\n",
    "agents_df = read_csv_to_df(spark, \"/casestudy/day176/hour20/sales_agents.csv\", infer_schema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153ca4f5-b2e6-4140-a0d5-a1a7b639c8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+-----------+--------------+--------------+--------------------+--------------+---------+----------+------------+----------------+-------+-------+-------+-------+-------+-----+----------+---------+--------------+----------------+-------------------+-----------+\n",
      "|   transaction_date|  transaction_id|customer_id|customer_fname|cusomter_lname|      cusomter_email|sales_agent_id|branch_id|product_id|product_name|product_category|offer_1|offer_2|offer_3|offer_4|offer_5|units|unit_price|is_online|payment_method|shipping_address|          load_date|load_source|\n",
      "+-------------------+----------------+-----------+--------------+--------------+--------------------+--------------+---------+----------+------------+----------------+-------+-------+-------+-------+-------+-----+----------+---------+--------------+----------------+-------------------+-----------+\n",
      "|2023-10-25 00:00:00|trx-072037549384|      85550|          Emma|        Wilson|emma.wilson@outlo...|           2.0|      3.0|         3|      Tablet|     Electronics|   null|   null|   null|   null|   null|    7|    299.99|       no|          Cash|            null|2024-06-24 00:00:00|    source1|\n",
      "|2022-05-08 00:00:00|trx-125208155197|      85512|        Olivia|        Miller|olivia.miller@out...|           9.0|      5.0|        11|          TV|     Electronics|   null|   null|   null|   null|   null|    3|    899.99|       no|   Credit Card|            null|2024-06-24 00:00:00|    source1|\n",
      "|2023-05-03 00:00:00|trx-667682345565|      85512|        Olivia|        Miller|olivia.miller@out...|           7.0|      2.0|        10|     Sandals|        Footwear|   null|   null|   null|   true|   null|    3|     39.99|       no|   Credit Card|            null|2024-06-24 00:00:00|    source1|\n",
      "|2022-01-09 00:00:00|trx-706068352444|      85526|        Sophia|         Smith|sophia.smith@hotm...|           2.0|      4.0|         2|  Smartphone|     Electronics|   null|   null|   null|   null|   true|    7|    699.99|       no|   Credit Card|            null|2024-06-24 00:00:00|    source1|\n",
      "|2022-12-10 00:00:00|trx-040134528974|      85535|       Michael|         Davis|michael.davis@out...|           9.0|      3.0|        16|       Skirt|        Clothing|   null|   null|   null|   true|   null|    7|     39.99|       no|          Cash|            null|2024-06-24 00:00:00|    source1|\n",
      "+-------------------+----------------+-----------+--------------+--------------+--------------------+--------------+---------+----------+------------+----------------+-------+-------+-------+-------+-------+-----+----------+---------+--------------+----------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d69c21a4-76ec-4f48-939e-de68acf19eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates\n",
    "transactions_df = remove_duplicates(transactions_df)\n",
    "branches_df = remove_duplicates(branches_df)\n",
    "agents_df = remove_duplicates(agents_df)\n",
    "\n",
    "# Checking for null values in key columns\n",
    "if check_nulls(transactions_df, \"transaction_id\"):\n",
    "    print(\"Null values found in transaction_id column\")\n",
    "if check_nulls(branches_df, \"branch_id\"):\n",
    "    print(\"Null values found in branch_id column\")\n",
    "if check_nulls(agents_df, \"sales_person_id\"):\n",
    "    print(\"Null values found in sales_person_id column\")\n",
    "\n",
    "# Calculating total paid price after discount and adding offer column\n",
    "transactions_df = calculate_total_paid_price_after_discount(transactions_df)\n",
    "transactions_df = add_offer_column(transactions_df)\n",
    "\n",
    "# Creating fact and dimension tables\n",
    "fact_sales = create_fact_sales(transactions_df)\n",
    "dim_sales = create_dim_sales(transactions_df)\n",
    "dim_product = create_dim_product(transactions_df)\n",
    "dim_customer = create_dim_customer(transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d2675c2-3dea-40c0-9e0a-6cb56b85c5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+--------------+---------+----------+-----+-----+----------+-------------------------------+\n",
      "|  transaction_id|customer_id|sales_agent_id|branch_id|product_id|offer|units|unit_price|total_paid_price_after_discount|\n",
      "+----------------+-----------+--------------+---------+----------+-----+-----+----------+-------------------------------+\n",
      "|trx-484141713642|      85482|          null|     null|        27|    5|    8|     29.99|                          29.99|\n",
      "|trx-027822757983|      85506|           7.0|      4.0|         2| null|    5|    699.99|                         699.99|\n",
      "|trx-905440336157|      85471|           4.0|      1.0|        21| null|    3|    129.99|                         129.99|\n",
      "|trx-422720630458|      85497|           7.0|      4.0|        14| null|    2|    399.99|                         399.99|\n",
      "|trx-321144022904|      85500|           9.0|      1.0|         5| null|    1|     19.99|                          19.99|\n",
      "+----------------+-----------+--------------+---------+----------+-----+-----+----------+-------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5f2344f-7f73-478a-8642-0b1239060f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- sales_agent_id: double (nullable = true)\n",
      " |-- branch_id: double (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- offer: string (nullable = true)\n",
      " |-- units: integer (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- total_paid_price_after_discount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_sales.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "901cbf25-c166-4db1-8f82-3602ad3904e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting data into casestudy.fact_sales.\n"
     ]
    }
   ],
   "source": [
    "insert_into_hive_table(spark, fact_sales, \"casestudy.fact_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e421b8cd-d7ce-4a58-8890-2c6facd1b2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table dim_product already exists. Inserting only new records.\n",
      "No new records to insert into dim_product.\n"
     ]
    }
   ],
   "source": [
    "insert_into_hive_table(spark, dim_product, \"dim_product\", primary_key=\"product_id\")  # Dim product - specify primary key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1233c2b5-8c4b-4647-aed1-2e17519e088d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
